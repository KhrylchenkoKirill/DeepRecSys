{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RecSys Course\n",
    "## Домашнее задание 1\n",
    "\n",
    "### ФИО: <впишите>\n",
    "\n",
    "### Введение\n",
    "В этом домашнем задании вы полностью пройдёте базовый пайплайн: подготовка данных → метрики → несколько рекомендательных подходов → итоговый лидерборд.\n",
    "\n",
    "### Используемые библиотеки\n",
    "В данном задании потребуются следующие библиотеки:\n",
    "* [polars](https://pola.rs/) - библиотека для работы с данными (человечество постепенно уходит от `pandas`'а)\n",
    "* [implicit](https://github.com/benfred/implicit) - библиотека для обучения и применения различных коллаборативных рекомендательных моделей\n",
    "* [torch](https://pytorch.org/) - no comments\n",
    "* [gensim](https://radimrehurek.com/gensim/) - обучение **word2vec**\n",
    "\n",
    "### Данные\n",
    "Данные лежат в архиве `data.zip`, который состоит из:\n",
    "* `interactions.parquet` - user-item взаимодействия из датасета Yambda (лайки для 500m версии)\n",
    "* `embeddings.parquet` - уже пофильтрованные и чуть более плотно запакованные эмбеддинги треков из Yambda\n",
    "* `artists.parquet` - метаданные айтемов с маппингом в артистов\n",
    "\n",
    "Скачать архив можно здесь: [ссылка на google disk](https://drive.google.com/file/d/1PojPVpXGBAqzHQi97QAFhJ9gnPsXxveS/view?usp=sharing). В следующем блоке мы в любом случае скачиваем датасет, поэтому самостоятельно его можно не качать.\n",
    "\n",
    "### Guidelines\n",
    "- Для выполнения ДЗ достаточно использовать Google Collab с T4\n",
    "- Детерминизм: фиксируйте сиды там, где это важно\n",
    "- Не используйте данные из теста при обучении/подготовке моделей\n",
    "- Тест — **последняя неделя** (по timestamp), как описано ниже\n",
    "- После каждого этапа запускайте проверки внутри ноутбука\n",
    "- Старайтесь избегать работы с сырыми питоновскими объектами (словарями, списками, интами) там, где можно применить методы из `polars` - они будут в десятки-сотни раз быстрее и читабельней\n",
    "- Чтобы быстрее обнаруживать, что код написан неоптимально и выполняется слишком долго, старайтесь оборачивать циклы в `tqdm` и выводить progress bar\n",
    "- Во время отладки кода можно посэмплировать данные для скорости с помощью `data.sample(fraction=0.1`. Для проверки тестов после отладки надо сделать запуски с полным датасетом\n",
    "\n",
    "### Разбалловка\n",
    "1. Подготовка данных (1 балл)\n",
    "2. Оценка качества (1 балл)\n",
    "3. Топ популярного (1 балл)\n",
    "4. Рекомендации по артистам (1 балл)\n",
    "5. Item-to-Item рекомендации (2 балла)\n",
    "6. Item2Vec (1 балл)\n",
    "7. Item-based Collaborative Filtering (1 балл)\n",
    "8. ALS (1 балл)\n",
    "9. Вопросы (1 балл)\n",
    "\n",
    "Суммарно - 10 баллов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests\n",
    "\n",
    "!pip install gensim\n",
    "# implicit устанавливается долго, минут 10\n",
    "!pip install implicit\n",
    "\n",
    "!pip install -q gdown\n",
    "!gdown --id 1PojPVpXGBAqzHQi97QAFhJ9gnPsXxveS -O dataset.zip\n",
    "!unzip -q dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Подготовка данных (1 балл)\n",
    "\n",
    "**Задача:**\n",
    "1) Считать данные (взаимодействия, эмбеддинги, метаданные).  \n",
    "2) Оставить только взаимодействия, для которых есть эмбеддинги.  \n",
    "3) Поджоинить метаданные (артисты треков) ко всем взаимодействиям.  \n",
    "4) Сделать core фильтрацию: оставить только айтемы с ≥5 взаимодействиями (почему мы это делаем -- исключительно для удобства и скорости, в реальной работе так делать не стоит)\n",
    "5) Сделать train-test split: последнюю неделю положить в тест.  \n",
    "6) Ограничить тест юзерами, у которых есть взаимодействия в трейне.  \n",
    "7) Подготовить для оценки качества `test_targets: Dict[uid, List[item_id]]`.\n",
    "\n",
    "После этого блока должны существовать `train`, `test`, `embeddings`, `artists`, `test_targets`.\n",
    "\n",
    "Для этого блока полезны как минимум следующие методы:\n",
    "* `pl.read_parquet` - для чтения данных\n",
    "* `.filter, .value_counts` помогут сделать core-фильтрацию\n",
    "* `df.join(...)` - при джойне метаданных надо использовать `how='left'`, а не `how='inner'`\n",
    "* `df.join(other, on=some_key, how='semi')` -- режим `semi` используется для фильтраций (оставить только те строки из исходного датафрейма, ключ которых присутствует во второй таблице)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import tests\n",
    "\n",
    "# Пути к данным (ожидается, что они лежат рядом с ноутбуком)\n",
    "DATA_DIR = \".\"\n",
    "PATH_INTERACTIONS = os.path.join(DATA_DIR, \"interactions.parquet\")\n",
    "PATH_EMBEDDINGS = os.path.join(DATA_DIR, \"embeddings.parquet\")\n",
    "PATH_ARTISTS = os.path.join(DATA_DIR, \"artists.parquet\")\n",
    "\n",
    "# Глобальные параметры\n",
    "TOPK = 100\n",
    "CORE_MIN_INTERACTIONS_PER_ITEM = 5\n",
    "TEST_INTERVAL_SECONDS = 7 * 24 * 60 * 60\n",
    "\n",
    "# Для воспроизводимости\n",
    "np.random.seed(42)\n",
    "\n",
    "data = pl.read_parquet(PATH_INTERACTIONS)\n",
    "embeddings = pl.read_parquet(PATH_EMBEDDINGS)\n",
    "artists = pl.read_parquet(PATH_ARTISTS)\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск автопроверок (для них необходим файл tests.py)\n",
    "\n",
    "tests.check_data_split(train=train, test=test, embeddings=embeddings, artists=artists, test_targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Оценка качества (1 балл)\n",
    "\n",
    "#### 2.1 Определения метрик\n",
    "\n",
    "2.1.1 Пусть для пользователя $u$:\n",
    "\n",
    "* $G_u \\subset \\mathcal{I}$ — множество релевантных айтемов (ground truth)\n",
    "* $R_u = (r_{u,1}, \\dots, r_{u,K})$ — упорядоченный список рекомендаций длины $K$\n",
    "\n",
    "Обозначим индикатор релевантности $I_{u,k} = [ r_{u, k} \\in G_u]$. В простонародье его еще часто называют `hits`.\n",
    "\n",
    "2.1.2 **Hitrate@K** равен единичке, если мы угадали в topK хотя бы один релевантный айтем:\n",
    "* $\n",
    "\\text{Hitrate@K} = \\frac{1}{|U|}\n",
    "\\sum_{u \\in U}\n",
    "\\left[ \\sum_{k=1}^{K} I_{u,k} > 0 \\right]\n",
    "$\n",
    "\n",
    "2.1.3 **Recall@K** оценивает долю угаданных релевантных айтемов (от всех релевантных айтемов):\n",
    "* $\n",
    "\\text{Recall@K} = \\frac{1}{|U|}\n",
    "\\sum_{u \\in U}\n",
    "\\frac{\n",
    "\\sum_{k=1}^{K} I_{u,k}\n",
    "}{\n",
    "\\min(|G_u|, K)\n",
    "}\n",
    "$\n",
    "\n",
    "2.1.4 Для подсчета **nDCG@K** нужно сначала посчитать **DCG@K**, затем посчитать **iDCG@K** (DCG в случае идеального ранжирования), затем одно поделить на другое:\n",
    "* $\n",
    "\\text{DCG@K}(u) = \\sum_{k=1}^{K}\n",
    "\\frac{I_{u,k}}{\\log_2(k+1)}\n",
    "$\n",
    "* $\n",
    "\\text{iDCG@K}(u) = \\sum_{k=1}^{\\min(|G_u|,K)}\n",
    "\\frac{1}{\\log_2(k+1)}\n",
    "$\n",
    "* $\n",
    "\\text{nDCG@K} = \\frac{1}{|U|}\n",
    "\\sum_{u \\in U}\n",
    "\\frac{\\text{DCG@K}(u)}{\\text{iDCG@K}(u)}\n",
    "$\n",
    "\n",
    "2.1.5 **Coverage@K** - это число уникальных айтемов во всех рекомендациях, деленное на размер каталога:\n",
    "\n",
    "* $\n",
    "\\text{Coverage@K} = \\frac{|\\bigcup_{u \\in U} R_u|}{|\\mathcal{I}_{train}|},\n",
    "$ где $\\mathcal{I}_{train}$ — каталог айтемов в train.\n",
    "* в качестве размера каталога используем количество айтемов, которые нам доступны для рекомендации на момент рекомендации (то есть количество уникальных айтемов в `train`)\n",
    "\n",
    "#### 2.2 Что нужно сделать\n",
    "Реализуйте функции:\n",
    "- `get_metrics(targets, candidates, topk) -> dict(hitrate, recall, ndcg)`\n",
    "- `evaluate(targets_by_user, candidates_by_user, catalog_size, topk) -> dict(hitrate, recall, ndcg, coverage)`\n",
    "\n",
    "**Важно:** `candidates[uid]` должен иметь длину ровно `topk` (и без `None`).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(targets: List[int], candidates: List[int], topk: int) -> Dict[str, float]:\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    targets: Dict[int, List[int]],\n",
    "    candidates: Dict[int, List[int]],\n",
    "    catalog_size: int,\n",
    "    topk: int = 100,\n",
    ") -> Dict[str, float]:\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_metrics(get_metrics=get_metrics, evaluate=evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Топ популярного (1 балл)\n",
    "\n",
    "Сделайте топ популярных айтемов по train взаимодействиям и посчитайте метрики с помощью `evaluate`.\n",
    "\n",
    "Полезные методы из `polars`: `.value_counts, .sort, .head, .to_numpy, .tolist, .n_unique`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нужно сложить результат метода evaluate в словарь metrics_toppop, далее в ноутбуке аналогично\n",
    "tests.check_top_pop(metrics_toppop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Рекомендации по артистам (1 балл)\n",
    "\n",
    "Что хотим:\n",
    "1. Взять последние N лайков пользователя (для моделей и всех подсчетов нужно использовать ТОЛЬКО `train`, тест используется исключительно для оценки качества)\n",
    "2. Посчитать по ним любимых артистов пользователя (отсортировать по количеству лайков у артиста)\n",
    "3. Взять у любимых артистов их самые популярные треки (чтобы посчитать популярность треков, нужно использовать train)\n",
    "4. Оставить те, которые пользователь еще не видел (не лайкал)\n",
    "5. Их порекомендовать\n",
    "\n",
    "Фактически мы формируем рекомендации по счётчикам - учитываем \"сколько раз пользователь лайкал артиста\" и \"сколько раз трек артиста был лайкнут\".\n",
    "\n",
    "В этом методе не для всех пользователей найдётся 100 кандидатов, поэтому нужно дополнить рекомендации до 100 каким-то другим методом, то есть сделать fallback. В данном случае предлагается делать fallback с помощью top_pop:\n",
    "* если у пользователя набралось меньше 100 рекомендаций, то идём по top_pop и добавляем кандидатов в рекомендации, пока не заполним до 100\n",
    "* при этом добавляем только unseen треки (те треки, которых у пользователя еще не было)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# именно такие значения ожидает автопроверка tests.check_artist_recs\n",
    "N_LAST_EVENTS = 100  # n последних взаимодействий\n",
    "PER_ARTIST_LIMIT = 20  # берем для рекомендаций 20 айтемов из каждого любимого артиста\n",
    "\n",
    "\n",
    "# эту функцию будем переиспользовать в следующих пунктах\n",
    "def fallback_to_toppop(cands_by_uid: Dict[int, List[int]], top_pop: np.ndarray, topk: int) -> Dict[int, List[int]]:\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    pass\n",
    "\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_artist_recs(metrics_artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Item-to-Item рекомендации (2 балла)\n",
    "\n",
    "Здесь предлагается реализовать классический item-to-item алгоритм:\n",
    "1. Считаем для каждого айтема список похожих айтемов\n",
    "2. Берём последние N взаимодействий пользователя, для каждого вытаскиваем список похожих\n",
    "3. Агрегируем кандидатов из списка похожих, учитывая суммарные похожести (если айтем встретился в двух списках кандидатов у пользователя, суммируем похожесть из каждого); при этом отфильтровываем все, что уже было у пользователя в истории\n",
    "4. Оставляем только topk кандидатов\n",
    "\n",
    "Реализация должна состоять из двух функций:\n",
    "1) `get_similar_items_gpu(item_ids, item_embeddings, topk_sim)` - для каждого айтема находим top similar items по cosine. \n",
    "* используем pytorch (и GPU) - иначе подсчет будет очень долгим\n",
    "* создаем тензор с эмбеддингами, переводим на GPU, $l2$-нормализуем\n",
    "* идём батчами по эмбеддингам, для каждого батча с помощью `torch.topk` считаем честный топ похожих; бачти -- это важно! сильно влияет на скорость\n",
    "* запоминаем эту информацию в словаре вида `item_id -> [(cand_item_id, similarity), ...]`\n",
    "\n",
    "2) `get_candidates_item2item(interactions, similar_items, ...)` - для каждого пользователя агрегируем похожести по последним N взаимодействиям.\n",
    "* нужно буквально реализовать выше описанный алгоритм (берём последние N взаимодействий, суммируем похожести всех полученных похожих айтемов)\n",
    "* для написания этой функции лучше не пытаться сделать супер оптимальный код через `polars` (у него очень большое пиковое потребление оперативной памяти), а как раз манипулировать словарями, списками, etc\n",
    "* для ускорения предлагается использовать `heapq.nlargest` для поиска `topk` элементов среди уже подсчитанных суммарных похожестей (раза в полтора-два быстрее, чем делать полную сортировку всех кандидатов со всех списков)\n",
    "\n",
    "В пункте 2 нужно дополнительно поддержать time decay:\n",
    "* свежие взаимодействия должны вносить более высокий вклад, поэтому при суммировании похожестей для конкретного кандидата мы учитываем свежесть события, из списка которого берём похожесть\n",
    "* вес события $2^{-\\frac{L-t}{\\tau}}$, где $L$ - длина истории пользователя, $t$ - позиция события в истории (начиная с единички), $\\tau$ - период полураспада, то есть насколько быстро затухает сигнал от событий. Например, при $\\tau = 10$ вес события падает в два раза, если оно идет на 10 позиций раньше в истории\n",
    "* Получаем $\\text{score}(u, j) =\n",
    "\\sum_{t=1}^{L}\n",
    "2^{- \\frac{L - t}{\\tau}}\n",
    "\\cdot\n",
    "s(i_{u,t}, j)\n",
    "$\n",
    "\n",
    "**Важно:** при использовании метода `get_similar_items_gpu` набираем не 100 кандидатов, а в два раза больше (2 * topk), чтобы после фильтрации по истории пользователя и слияния всех списков у нас было больше шансов набрать `topk`.\n",
    "\n",
    "Полезные методы из `polars`: `.to_torch()`, `.to_numpy()` - позволяют сразу получить тензоры для айдишников / эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import heapq\n",
    "\n",
    "def get_similar_items_gpu(\n",
    "    item_ids: np.ndarray,\n",
    "    item_embeddings: np.ndarray,\n",
    "    block: int = 1024,\n",
    "    topk: int = 200,\n",
    "    device: str = \"cuda\",\n",
    ") -> Dict[int, List[Tuple[int, float]]]:\n",
    "    \"\"\"\n",
    "    Возвращает словарь item_id -> [(cand_item_id, similarity), ...] длины topk\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_candidates_item2item(\n",
    "    interactions: pl.DataFrame,\n",
    "    similar_items: Dict[int, List[Tuple[int, float]]],\n",
    "    n_last: int = 30,\n",
    "    half_life_frac: float = 0.5,\n",
    "    topk: int = 100,\n",
    ") -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    half_life_frac интерпретируется как доля n_last: half_life = half_life_frac * n_last\n",
    "    w(r) = 2^{-r / half_life}, где r=0 для самого последнего события.\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_i2i_recs(metrics_i2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Item2Vec (1 балл)\n",
    "\n",
    "Item-to-item можно использовать с буквально любыми похожестями айтемов.\n",
    "\n",
    "Теперь давайте попробуем сами обучить эмбеддинги айтемов, посчитать похожести и опять применить item-to-item.\n",
    "\n",
    "Будем использовать подход Item2Vec - применяем подход word2vec к последовательностям айтемов вместо последовательностей слов.\n",
    "\n",
    "Для этого предлагается использовать `gensim`:\n",
    "* нужно создать `corpus` из списка списков строк вида `[['1', '2'], ['3', '4', '5']]`, в котором строки - это буквально строки айдишников айтемов, а внутренние списки - это сгруппированные взаимодействия пользователя (хронологически отсортированные)\n",
    "* Вызвать метод `gensim.models.Word2Vec`. Предлагаемые настройки `vector_size=100, window=5, min_count=1, sg=0, epochs=5`\n",
    "\n",
    "После обучения, эмбеддинги можно достать с помощью `w2v.wv.vectors` и соответствующие айдишники с помощью `w2v.wv.index_to_key`.\n",
    "\n",
    "Далее предлагается применить пайплайн из прошлого пункта: `get_similar_items_gpu` + `get_candidates_item2item`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_w2v_recs(metrics_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Item-based Collaborative Filtering (1 балл)\n",
    "\n",
    "Теперь попробуем применить тот же item-to-item подход, но используя в качестве векторов большие разреженные векторы из user-item матрицы. Для подсчета разреженных близостей наш GPU пайплайн не подходит (не умеет работать с разреженными данными), поэтому будем использовать библиотеку `implicit`\n",
    "\n",
    "1. Сначала нужно собрать CSR user-item матрицу из наших `train` взаимодействий с помощью `scipy.sparse.csr_matrix`\n",
    "* предлагается использовать формат вида `csr_matrix((ones, (user_ids, item_ids)), shape=(num_users, num_items))`\n",
    "* чтобы просуммировать взаимодействия с одними и теми же айтемами можно использовать `.sum_duplicates`\n",
    "* также понадобится от исходных айдишников (которые необязательно от 1 до `num_items / num_users`) перейти к компактным айдишникам, сделав маппинг {old_item_id: new_item_id}\n",
    "* простой вариант -- сделать это с помощью словаря, более быстрый - использовать метод вида `train.select(\"uid\").unique().with_row_index()` (аналогично для айтемов)\n",
    "\n",
    "2. Чтобы с помощью `implicit` получить списки похожих, нужно использовать комбинацию из `CosineRecommender.fit`, и `.similar_items`\n",
    "\n",
    "3. Чтобы сформировать рекомендации, используем нашу функцию `get_candidates_item2item`\n",
    "\n",
    "**Warning:** неаккуратно написанный код в этом пункте может переполнить оперативную память."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from implicit.nearest_neighbours import CosineRecommender, tfidf_weight\n",
    "\n",
    "\n",
    "def run_cosine(X: csr_matrix):\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    pass\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_cf_recs(metrics_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2 TF-IDF\n",
    "\n",
    "А теперь щепотка магии - с помощью `implicit.nearest_neighbours.tfidf_weight` модифицируем user-item матрицу и получим более высокие метрики.\n",
    "\n",
    "Все, что нужно -- это применить этот метод к матрице и затем проделать все те же операции, что и раньше (с вычислением похожестей и тд)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_weight(X)\n",
    "metrics_cf_tfidf = run_cosine(X_tfidf)\n",
    "\n",
    "metrics_cf_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_tfidf_recs(metrics_cf_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ALS (1 балл)\n",
    "\n",
    "С помощью `implicit.als.AlternatingLeastSquares` над той же самой разреженной user-item матрицей можно обучить ALS, что вам и предлагается сделать.\n",
    "\n",
    "Чтобы сформировать рекомендации, наша функция `get_candidates_item2item` не нужна -- достаточно использовать метод `model.recommend`.\n",
    "\n",
    "Нужно обучить ALS с двумя версиями user-item матриц - исходной и tfidf-модифицированной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "def run_als(X: csr_matrix, factors: int = 100, reg: float = 0.5, alpha: float = 0.1, iters: int = 20):\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    pass\n",
    "\n",
    "metrics_als_raw = run_als(X)\n",
    "metrics_als_tfidf = run_als(X_tfidf)\n",
    "\n",
    "metrics_als_raw, metrics_als_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.check_als_recs(metrics_als_raw, metrics_als_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Лидерборд и выводы\n",
    "\n",
    "Собираем таблицу со всеми методами и метриками.\n",
    "\n",
    "Добавьте 5–10 строк выводов к экспериментам: что работает лучше и почему.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "leaderboard = pd.DataFrame([\n",
    "    {\"method\": \"TopPop\", **metrics_toppop},\n",
    "    {\"method\": \"User-Artist\", **metrics_artist},\n",
    "    {\"method\": \"Item2Item (dataset emb)\", **metrics_i2i},\n",
    "    {\"method\": \"Item2Vec (w2v)\", **metrics_w2v},\n",
    "    {\"method\": \"CF Cosine (raw)\", **metrics_cf},\n",
    "    {\"method\": \"CF Cosine (tf-idf)\", **metrics_cf_tfidf},\n",
    "    {\"method\": \"ALS (raw)\", **metrics_als_raw},\n",
    "    {\"method\": \"ALS (tf-idf)\", **metrics_als_tfidf},\n",
    "])\n",
    "\n",
    "leaderboard = leaderboard.sort_values([\"recall\", \"ndcg\"], ascending=False)\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопросы на понимание (1 балл)\n",
    "\n",
    "1) Почему для кандидатов/метрик важно исключать айтемы, которые пользователь уже видел?  \n",
    "2) Почему при джойне метаданных (да и вообще почти при любом джойне) нужно использовать how='left', а не how='inner'?\n",
    "3) В рекомендациях по артистам (с помощью счётчиков) не для всех пользователей может найтись нужное количество кандидатов. Почему?\n",
    "4) Почему tf-idf улучшает item-based CF? На саму функцию можно посмотреть через `tfidf_weight??`\n",
    "5) В чем принципиальное отличие между item-to-item методом и методом, при котором мы получаем эмбеддинг пользователя, сложив эмбеддинги его последних взаимодействий, и затем ищем ближайшие эмбеддинги айтемов?\n",
    "5) Почему ALS выиграл у чистого cosine-item2item?\n",
    "\n",
    "Ответы - текстом"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
