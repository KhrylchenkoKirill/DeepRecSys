{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dee380d7",
      "metadata": {
        "id": "dee380d7"
      },
      "source": [
        "# Yambda Dataset: Used Benchmarks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libaries import"
      ],
      "metadata": {
        "id": "vv-RNk9bThal"
      },
      "id": "vv-RNk9bThal"
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y -qq --no-install-recommends libsuitesparse-dev build-essential git gfortran\n",
        "!git clone https://github.com/glami/sansa.git\n",
        "!pip install ./sansa && rm -rf sansa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xIt_PPI9mNJb",
        "outputId": "2db9e80b-9957-4a3b-fc6b-c28c92bc9605"
      },
      "id": "xIt_PPI9mNJb",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Cloning into 'sansa'...\n",
            "remote: Enumerating objects: 609, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 609 (delta 64), reused 84 (delta 49), pack-reused 466 (from 1)\u001b[K\n",
            "Receiving objects: 100% (609/609), 1.80 MiB | 5.31 MiB/s, done.\n",
            "Resolving deltas: 100% (358/358), done.\n",
            "Processing ./sansa\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.12/dist-packages (from sansa==1.1.0) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from sansa==1.1.0) (2.0.2)\n",
            "Requirement already satisfied: scikit-sparse>=0.4.8 in /usr/local/lib/python3.12/dist-packages (from sansa==1.1.0) (0.4.16)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.12/dist-packages (from sansa==1.1.0) (1.16.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.57.0->sansa==1.1.0) (0.43.0)\n",
            "Building wheels for collected packages: sansa\n",
            "  Building wheel for sansa (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sansa: filename=sansa-1.1.0-py3-none-any.whl size=31025 sha256=0ea61877ebb7429789b76d2bc9ee9c38e91f83df61088ebf3e47d9948a512035\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6aen37od/wheels/d1/4a/9f/764b5b8eef0ddba21c63f6512775f8347a63631de885550093\n",
            "Successfully built sansa\n",
            "Installing collected packages: sansa\n",
            "  Attempting uninstall: sansa\n",
            "    Found existing installation: sansa 1.1.0\n",
            "    Uninstalling sansa-1.1.0:\n",
            "      Successfully uninstalled sansa-1.1.0\n",
            "Successfully installed sansa-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bf3db056",
      "metadata": {
        "id": "bf3db056"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import defaultdict\n",
        "import dataclasses\n",
        "import functools\n",
        "from functools import cached_property\n",
        "import heapq\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Iterable, Literal\n",
        "\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import scipy.sparse as sp\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import Dataset, DatasetDict, load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ac350353",
      "metadata": {
        "id": "ac350353"
      },
      "outputs": [],
      "source": [
        "from sansa import SANSA, ICFGramianFactorizerConfig, SANSAConfig, UMRUnitLowerTriangleInverterConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a764d1ff",
      "metadata": {
        "id": "a764d1ff"
      },
      "source": [
        "# Utils & Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b2a80f35",
      "metadata": {
        "id": "b2a80f35"
      },
      "outputs": [],
      "source": [
        "class Constants:\n",
        "    HOUR_SECONDS = 60 * 60\n",
        "    DAY_SECONDS = 24 * HOUR_SECONDS\n",
        "\n",
        "    GAP_SIZE = HOUR_SECONDS // 2\n",
        "    VAL_SIZE = 1 * DAY_SECONDS\n",
        "    TEST_SIZE = 1 * DAY_SECONDS\n",
        "\n",
        "    LAST_TIMESTAMP = 26000000\n",
        "    TEST_TIMESTAMP = LAST_TIMESTAMP - TEST_SIZE\n",
        "\n",
        "    TRACK_LISTEN_THRESHOLD = 50\n",
        "\n",
        "    NUM_RANKED_ITEMS = 100\n",
        "\n",
        "    METRICS = [\n",
        "        \"ndcg@10\",\n",
        "        \"ndcg@100\",\n",
        "        \"recall@10\",\n",
        "        \"recall@100\",\n",
        "        \"coverage@10\",\n",
        "        \"coverage@100\",\n",
        "    ]\n",
        "\n",
        "    IDEAL_METRICS = {\n",
        "        'most_pop': {\n",
        "            \"ndcg@10\": 0.0046,\n",
        "            \"ndcg@100\": 0.0097,\n",
        "            \"recall@10\": 0.0083,\n",
        "            \"recall@100\": 0.0222,\n",
        "            \"coverage@10\": 0.0001,\n",
        "            \"coverage@100\": 0.0006,\n",
        "        },\n",
        "        'decay_pop': {\n",
        "            \"ndcg@10\": 0.0180,\n",
        "            \"ndcg@100\": 0.0269,\n",
        "            \"recall@10\": 0.0333,\n",
        "            \"recall@100\": 0.0651,\n",
        "            \"coverage@10\": 0.0001,\n",
        "            \"coverage@100\": 0.0006,\n",
        "        },\n",
        "        'item_knn': {\n",
        "            \"ndcg@10\": 0.0125,\n",
        "            \"ndcg@100\": 0.0251,\n",
        "            \"recall@10\": 0.0199,\n",
        "            \"recall@100\": 0.0648,\n",
        "            \"coverage@10\": 0.1050,\n",
        "            \"coverage@100\": 0.3922,\n",
        "        },\n",
        "        'sansa': {\n",
        "            \"ndcg@10\": 0.0068,\n",
        "            \"ndcg@100\": 0.0203,\n",
        "            \"recall@10\": 0.0105,\n",
        "            \"recall@100\": 0.0616,\n",
        "            \"coverage@10\": 0.0194,\n",
        "            \"coverage@100\": 0.1601,\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d47d7d7a",
      "metadata": {
        "id": "d47d7d7a"
      },
      "outputs": [],
      "source": [
        "def rank_items(users: Embeddings, items: Embeddings, num_items: int, batch_size: int = 128) -> Ranked:\n",
        "    assert users.device == items.device\n",
        "\n",
        "    num_users = users.ids.shape[0]\n",
        "\n",
        "    scores = users.embeddings.new_empty((num_users, num_items))\n",
        "    item_ids = users.embeddings.new_empty((num_users, num_items), dtype=torch.long)\n",
        "\n",
        "    for batch_idx in tqdm(range((num_users + batch_size - 1) // batch_size), desc=\"Calc topk by batches\"):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = (batch_idx + 1) * batch_size\n",
        "\n",
        "        batch_scores = users.embeddings[start_idx:end_idx, :] @ items.embeddings.T\n",
        "\n",
        "        sort_indices = batch_scores.topk(num_items, dim=-1).indices\n",
        "        scores[start_idx:end_idx, :] = torch.gather(batch_scores, dim=-1, index=sort_indices)\n",
        "\n",
        "        item_ids[start_idx:end_idx, :] = torch.gather(\n",
        "            items.ids.expand(sort_indices.shape[0], items.ids.shape[0]), dim=-1, index=sort_indices\n",
        "        )\n",
        "\n",
        "    return Ranked(user_ids=users.ids, item_ids=item_ids, scores=scores, num_item_ids=items.ids.shape[0])\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Ranked:\n",
        "    user_ids: torch.Tensor\n",
        "    item_ids: torch.Tensor\n",
        "    scores: torch.Tensor | None = None\n",
        "    num_item_ids: int | None = None  # number of all items. Useful for coverage and etc.\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.scores is None:\n",
        "            self.scores = torch.arange(\n",
        "                self.item_ids.shape[1], 0, -1, device=self.item_ids.device, dtype=torch.float32\n",
        "            ).expand((self.user_ids.shape[0], self.item_ids.shape[1]))\n",
        "\n",
        "        assert self.user_ids.dim() == 1\n",
        "        assert self.scores.dim() == 2\n",
        "        assert self.scores.shape == self.item_ids.shape\n",
        "        assert self.user_ids.shape[0] == self.scores.shape[0]\n",
        "\n",
        "        assert self.user_ids.device == self.scores.device == self.item_ids.device\n",
        "\n",
        "        assert torch.all(self.scores[:, :-1] >= self.scores[:, 1:]), \"scores should be sorted\"\n",
        "\n",
        "        if not torch.all(self.user_ids[:-1] <= self.user_ids[1:]):\n",
        "            indexes = torch.argsort(self.user_ids, descending=False)\n",
        "            self.item_ids = self.item_ids[indexes, :]\n",
        "            self.scores = self.scores[indexes, :]\n",
        "            self.user_ids = self.user_ids[indexes]\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.user_ids.device\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Targets:\n",
        "    user_ids: torch.Tensor\n",
        "    item_ids: list[torch.Tensor]\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert len(self.item_ids) > 0\n",
        "        assert self.user_ids.dim() == 1\n",
        "        assert self.user_ids.shape[0] == len(self.item_ids)\n",
        "        assert all(x.dim() == 1 for x in self.item_ids), \"all ids should be 1D\"\n",
        "\n",
        "        assert all(x.device == self.item_ids[0].device for x in self.item_ids), \"all ids should be on the same device\"\n",
        "        assert self.user_ids.device == self.item_ids[0].device\n",
        "\n",
        "        if not torch.all(self.user_ids[:-1] <= self.user_ids[1:]):\n",
        "            indexes = torch.argsort(self.user_ids, descending=False)\n",
        "            self.item_ids = [self.item_ids[i] for i in indexes]\n",
        "            self.user_ids = self.user_ids[indexes]\n",
        "\n",
        "        assert torch.all(self.user_ids[:-1] < self.user_ids[1:]), \"user_ids should be unique\"\n",
        "\n",
        "    @cached_property\n",
        "    def lengths(self):\n",
        "        return torch.tensor([ids.shape[0] for ids in self.item_ids], device=self.item_ids[0].device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.item_ids)\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.user_ids.device\n",
        "\n",
        "    @classmethod\n",
        "    def from_sequential(cls, df: pl.LazyFrame | pl.DataFrame, device: torch.device | str) -> 'Targets':\n",
        "        return cls(\n",
        "            df.select(\"uid\")[\"uid\"].to_torch().to(device),\n",
        "            [torch.tensor(x, device=device) for x in df.select(\"item_id\")[\"item_id\"].to_list()],\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Embeddings:\n",
        "    ids: torch.Tensor\n",
        "    embeddings: torch.Tensor\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.ids.dim() == 1\n",
        "        assert self.embeddings.dim() == 2\n",
        "        assert self.ids.shape[0] == self.embeddings.shape[0]\n",
        "\n",
        "        assert self.ids.device == self.embeddings.device\n",
        "\n",
        "        if not torch.all(self.ids[:-1] <= self.ids[1:]):\n",
        "            indexes = torch.argsort(self.ids, descending=False)\n",
        "            self.embeddings = self.embeddings[indexes, :]\n",
        "            self.ids = self.ids[indexes]\n",
        "\n",
        "        assert torch.all(self.ids[:-1] < self.ids[1:]), \"ids should be unique\"\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.ids.device\n",
        "\n",
        "    def save(self, file_path: str):\n",
        "        ids_np = self.ids.cpu().numpy()\n",
        "        embeddings_np = self.embeddings.cpu().numpy()\n",
        "        np.savez(file_path, ids=ids_np, embeddings=embeddings_np)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file_path: str, device: torch.device = torch.device('cpu')) -> 'Embeddings':\n",
        "        with np.load(file_path) as data:\n",
        "            ids_np = data['ids']\n",
        "            embeddings_np = data['embeddings']\n",
        "\n",
        "        ids = torch.from_numpy(ids_np).to(device)\n",
        "        embeddings = torch.from_numpy(embeddings_np).to(device)\n",
        "\n",
        "        return cls(ids=ids, embeddings=embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "Z_GK_OMnQKcO"
      },
      "id": "Z_GK_OMnQKcO"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c6c41fbd",
      "metadata": {
        "id": "c6c41fbd"
      },
      "outputs": [],
      "source": [
        "def cut_off_ranked(ranked: Ranked, targets: Targets) -> Ranked:\n",
        "    mask = torch.isin(ranked.user_ids, targets.user_ids)\n",
        "\n",
        "    assert ranked.scores is not None\n",
        "\n",
        "    ranked = Ranked(\n",
        "        user_ids=ranked.user_ids[mask],\n",
        "        scores=ranked.scores[mask, :],\n",
        "        item_ids=ranked.item_ids[mask, :],\n",
        "        num_item_ids=ranked.num_item_ids,\n",
        "    )\n",
        "\n",
        "    assert ranked.item_ids.shape[0] == len(targets), \"Ranked doesn't contain all targets.user_ids\"\n",
        "\n",
        "    return ranked\n",
        "\n",
        "\n",
        "class Metric(ABC):\n",
        "    @abstractmethod\n",
        "    def __call__(\n",
        "        self, ranked: Ranked | None, targets: Targets | None, target_mask: torch.Tensor | None, ks: Iterable[int]\n",
        "    ) -> dict[int, float]:\n",
        "        pass\n",
        "\n",
        "\n",
        "class Recall(Metric):\n",
        "    def __call__(\n",
        "        self, ranked: Ranked | None, targets: Targets, target_mask: torch.Tensor, ks: Iterable[int]\n",
        "    ) -> dict[int, float]:\n",
        "        assert all(0 < k <= target_mask.shape[1] for k in ks)\n",
        "\n",
        "        values = {}\n",
        "\n",
        "        for k in ks:\n",
        "            num_positives = targets.lengths.to(torch.float32)\n",
        "            num_positives[num_positives == 0] = torch.inf\n",
        "\n",
        "            values[k] = target_mask[:, :k].to(torch.float32).sum(dim=-1) / num_positives\n",
        "\n",
        "            values[k] = torch.mean(values[k]).item()\n",
        "\n",
        "        return values\n",
        "\n",
        "\n",
        "class DCG(Metric):\n",
        "    def __call__(\n",
        "        self, ranked: Ranked | None, targets: Targets | None, target_mask: torch.Tensor, ks: Iterable[int]\n",
        "    ) -> dict[int, float]:\n",
        "        assert all(0 < k <= target_mask.shape[1] for k in ks)\n",
        "\n",
        "        values = {}\n",
        "\n",
        "        discounts = 1.0 / torch.log2(\n",
        "            torch.arange(2, target_mask.shape[1] + 2, device=target_mask.device, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "        for k in ks:\n",
        "            dcg_k = torch.sum(target_mask[:, :k] * discounts[:k], dim=1)\n",
        "            values[k] = torch.mean(dcg_k).item()\n",
        "\n",
        "        return values\n",
        "\n",
        "\n",
        "class NDCG(Metric):\n",
        "    def __call__(\n",
        "        self, ranked: Ranked | None, targets: Targets, target_mask: torch.Tensor, ks: Iterable[int]\n",
        "    ) -> dict[int, float]:\n",
        "        actual_dcg = DCG()(ranked, targets, target_mask, ks)\n",
        "\n",
        "        ideal_target_mask = (\n",
        "            torch.arange(target_mask.shape[1], device=targets.device)[None, :] < targets.lengths[:, None]\n",
        "        ).to(torch.float32)\n",
        "        assert target_mask.shape == ideal_target_mask.shape\n",
        "\n",
        "        ideal_dcg = DCG()(ranked, targets, ideal_target_mask, ks)\n",
        "\n",
        "        ndcg_values = {k: (actual_dcg[k] / ideal_dcg[k] if ideal_dcg[k] != 0 else 0.0) for k in ks}\n",
        "\n",
        "        return ndcg_values\n",
        "\n",
        "\n",
        "class Coverage(Metric):\n",
        "    def __init__(self, cut_off_ranked: bool = False):\n",
        "        self.cut_off_ranked = cut_off_ranked\n",
        "\n",
        "    def __call__(\n",
        "        self, ranked: Ranked, targets: Targets | None, target_mask: torch.Tensor | None, ks: Iterable[int]\n",
        "    ) -> dict[int, float]:\n",
        "        if self.cut_off_ranked:\n",
        "            assert targets is not None\n",
        "            ranked = cut_off_ranked(ranked, targets)\n",
        "\n",
        "        assert all(0 < k <= ranked.item_ids.shape[1] for k in ks)\n",
        "\n",
        "        assert ranked.num_item_ids is not None\n",
        "\n",
        "        values = {}\n",
        "        for k in ks:\n",
        "            values[k] = ranked.item_ids[:, :k].flatten().unique().shape[0] / ranked.num_item_ids\n",
        "\n",
        "        return values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Utilities"
      ],
      "metadata": {
        "id": "4wgon9-qVDMD"
      },
      "id": "4wgon9-qVDMD"
    },
    {
      "cell_type": "code",
      "source": [
        "REGISTERED_METRIC_FN = {\n",
        "    \"recall\": Recall(),\n",
        "    \"ndcg\": NDCG(),\n",
        "    \"coverage\": Coverage(cut_off_ranked=False),\n",
        "}\n",
        "\n",
        "\n",
        "def calc_metrics(ranked: Ranked, targets: Targets, metrics: list[str]) -> dict[str, Any]:\n",
        "    grouped_metrics = _parse_metrics(metrics)\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    target_mask = create_target_mask(ranked, targets)\n",
        "\n",
        "    for name, ks in grouped_metrics.items():\n",
        "        result[name] = REGISTERED_METRIC_FN[name](ranked, targets, target_mask, ks=ks)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def _parse_metrics(metric_names: list[str]) -> dict[str, list[int]]:\n",
        "    parsed_metrics = []\n",
        "\n",
        "    for metric in metric_names:\n",
        "        parts = metric.split('@')\n",
        "        name = parts[0]\n",
        "\n",
        "        assert len(parts) > 1, f\"Invalid metric: {metric}, specify @k\"\n",
        "\n",
        "        value = int(parts[1])\n",
        "        parsed_metrics.append((name, value))\n",
        "\n",
        "    metrics = defaultdict(list)\n",
        "    for m in parsed_metrics:\n",
        "        metrics[m[0]].append(m[1])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def create_target_mask(ranked: Ranked, targets: Targets) -> torch.Tensor:\n",
        "    ranked = cut_off_ranked(ranked, targets)\n",
        "\n",
        "    assert ranked.device == targets.device\n",
        "    assert ranked.item_ids.shape[0] == len(targets)\n",
        "\n",
        "    target_mask = ranked.item_ids.new_zeros(ranked.item_ids.shape, dtype=torch.float32)\n",
        "\n",
        "    for i, target in enumerate(tqdm(targets.item_ids, desc=\"Making target mask\")):\n",
        "        target_mask[i, torch.isin(ranked.item_ids[i], target)] = 1.0\n",
        "\n",
        "    return target_mask"
      ],
      "metadata": {
        "id": "GsJGbuCwVFpA"
      },
      "id": "GsJGbuCwVFpA",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f3eb4dab",
      "metadata": {
        "id": "f3eb4dab"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "59fec80a",
      "metadata": {
        "id": "59fec80a"
      },
      "outputs": [],
      "source": [
        "class YambdaDataset:\n",
        "    INTERACTIONS = frozenset([\n",
        "        \"likes\", \"listens\", \"multi_event\", \"dislikes\", \"unlikes\", \"undislikes\"\n",
        "    ])\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_type: Literal[\"flat\", \"sequential\"] = \"flat\",\n",
        "        dataset_size: Literal[\"50m\", \"500m\", \"5b\"] = \"50m\"\n",
        "    ):\n",
        "        assert dataset_type in {\"flat\", \"sequential\"}\n",
        "        assert dataset_size in {\"50m\", \"500m\", \"5b\"}\n",
        "        self.dataset_type = dataset_type\n",
        "        self.dataset_size = dataset_size\n",
        "\n",
        "    def interaction(self, event_type: Literal[\n",
        "        \"likes\", \"listens\", \"multi_event\", \"dislikes\", \"unlikes\", \"undislikes\"\n",
        "    ]) -> Dataset:\n",
        "        assert event_type in YambdaDataset.INTERACTIONS\n",
        "        return self._download(f\"{self.dataset_type}/{self.dataset_size}\", event_type)\n",
        "\n",
        "    def audio_embeddings(self) -> Dataset:\n",
        "        return self._download(\"\", \"embeddings\")\n",
        "\n",
        "    def album_item_mapping(self) -> Dataset:\n",
        "        return self._download(\"\", \"album_item_mapping\")\n",
        "\n",
        "    def artist_item_mapping(self) -> Dataset:\n",
        "        return self._download(\"\", \"artist_item_mapping\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _download(data_dir: str, file: str) -> Dataset:\n",
        "        data = load_dataset(\"yandex/yambda\", data_dir=data_dir, data_files=f\"{file}.parquet\")\n",
        "        # Returns DatasetDict; extracting the only split\n",
        "        assert isinstance(data, DatasetDict)\n",
        "        return data[\"train\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5ea3f153",
      "metadata": {
        "id": "5ea3f153",
        "outputId": "78eb05f8-b4c8-4971-e0ac-10943119f576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 881,456 records (likes)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 4)\n",
              "┌─────┬───────────┬─────────┬────────────┐\n",
              "│ uid ┆ timestamp ┆ item_id ┆ is_organic │\n",
              "│ --- ┆ ---       ┆ ---     ┆ ---        │\n",
              "│ u32 ┆ u32       ┆ u32     ┆ u8         │\n",
              "╞═════╪═══════════╪═════════╪════════════╡\n",
              "│ 100 ┆ 44755     ┆ 732449  ┆ 1          │\n",
              "│ 100 ┆ 1155860   ┆ 6568592 ┆ 0          │\n",
              "│ 100 ┆ 1259125   ┆ 5411243 ┆ 1          │\n",
              "│ 100 ┆ 1260005   ┆ 7371186 ┆ 0          │\n",
              "│ 100 ┆ 1263935   ┆ 4943655 ┆ 0          │\n",
              "└─────┴───────────┴─────────┴────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>uid</th><th>timestamp</th><th>item_id</th><th>is_organic</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u8</td></tr></thead><tbody><tr><td>100</td><td>44755</td><td>732449</td><td>1</td></tr><tr><td>100</td><td>1155860</td><td>6568592</td><td>0</td></tr><tr><td>100</td><td>1259125</td><td>5411243</td><td>1</td></tr><tr><td>100</td><td>1260005</td><td>7371186</td><td>0</td></tr><tr><td>100</td><td>1263935</td><td>4943655</td><td>0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Load data using YambdaDataset\n",
        "yambda = YambdaDataset(dataset_type=\"flat\", dataset_size=\"50m\")\n",
        "likes_ds = yambda.interaction(\"likes\")  # load all likes\n",
        "print(f\"Loaded {len(likes_ds):,} records (likes)\")\n",
        "\n",
        "likes_df = likes_ds.to_polars()\n",
        "likes_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "42a32120",
      "metadata": {
        "id": "42a32120"
      },
      "outputs": [],
      "source": [
        "def flat_split_train_val_test(\n",
        "    df: pl.LazyFrame,\n",
        "    test_timestamp: int,\n",
        "    val_size: int = 0,\n",
        "    gap_size: int = Constants.GAP_SIZE,\n",
        "    drop_non_train_items: bool = False,\n",
        "    engine: str = \"streaming\",\n",
        ") -> tuple[pl.LazyFrame, pl.LazyFrame | None, pl.LazyFrame]:\n",
        "\n",
        "    # Filter out new items\n",
        "    def drop(df: pl.LazyFrame, unique_train_item_ids) -> pl.LazyFrame:\n",
        "        if not drop_non_train_items:\n",
        "            return df\n",
        "\n",
        "        return (\n",
        "            df.with_columns(\n",
        "                pl.col(\"item_id\").is_in(unique_train_item_ids.get_column(\"item_id\").implode()).alias(\"item_id_in_train\")\n",
        "            )\n",
        "            .filter(\"item_id_in_train\")\n",
        "            .drop(\"item_id_in_train\")\n",
        "        )\n",
        "\n",
        "    train_timestamp = test_timestamp - gap_size - val_size - (gap_size if val_size != 0 else 0)\n",
        "\n",
        "    assert gap_size >= 0\n",
        "    assert val_size >= 0\n",
        "    assert train_timestamp > 0\n",
        "\n",
        "    df_lazy = df.lazy()\n",
        "\n",
        "    # Create train subset\n",
        "    train = df_lazy.filter(pl.col(\"timestamp\") < train_timestamp)\n",
        "\n",
        "    # Warm-start setup\n",
        "    unique_train_uids = train.select(\"uid\").unique().collect(engine=engine)\n",
        "    unique_train_item_ids = train.select(\"item_id\").unique().collect(engine=engine)\n",
        "\n",
        "    # Create validation subset\n",
        "    validation = None\n",
        "    if val_size != 0:\n",
        "        validation = (\n",
        "            df_lazy.filter(\n",
        "                (pl.col(\"timestamp\") >= test_timestamp - val_size - gap_size)\n",
        "                & (pl.col(\"timestamp\") < test_timestamp - gap_size)\n",
        "            )\n",
        "            #\n",
        "            .with_columns(\n",
        "                pl.col(\"uid\").is_in(unique_train_uids.get_column(\"uid\").implode()).alias(\"uid_in_train\")\n",
        "            )  # to prevent filter reordering\n",
        "            .filter(\"uid_in_train\")\n",
        "            .drop(\"uid_in_train\")\n",
        "        )\n",
        "        validation = drop(validation, unique_train_item_ids)\n",
        "\n",
        "    # Create evaluation subset\n",
        "    test = (\n",
        "        df_lazy.filter(pl.col(\"timestamp\") >= test_timestamp)\n",
        "        #\n",
        "        .with_columns(\n",
        "            pl.col(\"uid\").is_in(unique_train_uids.get_column(\"uid\").implode()).alias(\"uid_in_train\")\n",
        "        )  # to prevent filter reordering\n",
        "        .filter(\"uid_in_train\")\n",
        "        .drop(\"uid_in_train\")\n",
        "    )\n",
        "    test = drop(test, unique_train_item_ids)\n",
        "\n",
        "    return train, validation, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "26e5499c",
      "metadata": {
        "id": "26e5499c"
      },
      "outputs": [],
      "source": [
        "train_likes_df, _, test_likes_df = flat_split_train_val_test(\n",
        "    df=likes_df,\n",
        "    test_timestamp=Constants.TEST_TIMESTAMP,\n",
        "    val_size=0,  # No validation is needed\n",
        "    gap_size=Constants.GAP_SIZE\n",
        ")\n",
        "\n",
        "train_likes_df = train_likes_df.collect()\n",
        "test_likes_df = test_likes_df.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "99d93751",
      "metadata": {
        "id": "99d93751",
        "outputId": "8513adc8-d0fa-405f-e3be-aa099681653d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 4)\n",
              "┌─────┬───────────┬─────────┬────────────┐\n",
              "│ uid ┆ timestamp ┆ item_id ┆ is_organic │\n",
              "│ --- ┆ ---       ┆ ---     ┆ ---        │\n",
              "│ u32 ┆ u32       ┆ u32     ┆ u8         │\n",
              "╞═════╪═══════════╪═════════╪════════════╡\n",
              "│ 100 ┆ 44755     ┆ 732449  ┆ 1          │\n",
              "│ 100 ┆ 1155860   ┆ 6568592 ┆ 0          │\n",
              "│ 100 ┆ 1259125   ┆ 5411243 ┆ 1          │\n",
              "│ 100 ┆ 1260005   ┆ 7371186 ┆ 0          │\n",
              "│ 100 ┆ 1263935   ┆ 4943655 ┆ 0          │\n",
              "└─────┴───────────┴─────────┴────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>uid</th><th>timestamp</th><th>item_id</th><th>is_organic</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u8</td></tr></thead><tbody><tr><td>100</td><td>44755</td><td>732449</td><td>1</td></tr><tr><td>100</td><td>1155860</td><td>6568592</td><td>0</td></tr><tr><td>100</td><td>1259125</td><td>5411243</td><td>1</td></tr><tr><td>100</td><td>1260005</td><td>7371186</td><td>0</td></tr><tr><td>100</td><td>1263935</td><td>4943655</td><td>0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_likes_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c575a808",
      "metadata": {
        "id": "c575a808",
        "outputId": "915bfdb2-4684-4bf4-ba6b-3ff34f63e7cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 4)\n",
              "┌──────┬───────────┬─────────┬────────────┐\n",
              "│ uid  ┆ timestamp ┆ item_id ┆ is_organic │\n",
              "│ ---  ┆ ---       ┆ ---     ┆ ---        │\n",
              "│ u32  ┆ u32       ┆ u32     ┆ u8         │\n",
              "╞══════╪═══════════╪═════════╪════════════╡\n",
              "│ 700  ┆ 25939545  ┆ 3383415 ┆ 0          │\n",
              "│ 1400 ┆ 25993865  ┆ 7707460 ┆ 1          │\n",
              "│ 1400 ┆ 25993945  ┆ 6005845 ┆ 1          │\n",
              "│ 1400 ┆ 25993945  ┆ 7539311 ┆ 0          │\n",
              "│ 1400 ┆ 25993975  ┆ 8908728 ┆ 1          │\n",
              "└──────┴───────────┴─────────┴────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>uid</th><th>timestamp</th><th>item_id</th><th>is_organic</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u8</td></tr></thead><tbody><tr><td>700</td><td>25939545</td><td>3383415</td><td>0</td></tr><tr><td>1400</td><td>25993865</td><td>7707460</td><td>1</td></tr><tr><td>1400</td><td>25993945</td><td>6005845</td><td>1</td></tr><tr><td>1400</td><td>25993945</td><td>7539311</td><td>0</td></tr><tr><td>1400</td><td>25993975</td><td>8908728</td><td>1</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "test_likes_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad2f4e09",
      "metadata": {
        "id": "ad2f4e09"
      },
      "source": [
        "### MostPop & DecayPop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aeee96b",
      "metadata": {
        "id": "6aeee96b"
      },
      "outputs": [],
      "source": [
        "def training_popular(\n",
        "        train_df: pl.DataFrame,\n",
        "        hour: float,\n",
        "        max_timestamp: float,\n",
        "        device: str,\n",
        "        decay: float = 0.9\n",
        "    ) -> Embeddings:\n",
        "    if hour == 0:  # MostPop option\n",
        "        embeddings = train_df.group_by(\"item_id\").agg(pl.count().alias(\"item_embedding\"))\n",
        "    else:  # DecayPop option\n",
        "        tau = decay ** (1 / Constants.DAY_SECONDS / (hour / 24))\n",
        "\n",
        "        embeddings = (\n",
        "            train_df.select(\n",
        "                \"item_id\",\n",
        "                (tau ** (max_timestamp - pl.col(\"timestamp\"))).alias(\"value\"),\n",
        "            )\n",
        "            .group_by(\"item_id\")\n",
        "            .agg(pl.col(\"value\").sum().alias(\"item_embedding\"))\n",
        "        )\n",
        "\n",
        "    item_ids = embeddings[\"item_id\"].to_torch().to(device)\n",
        "    item_embeddings = embeddings[\"item_embedding\"].to_torch().to(device)[:, None]\n",
        "\n",
        "    return Embeddings(item_ids, item_embeddings)\n",
        "\n",
        "\n",
        "def evaluation_popular(\n",
        "    train_df: pl.DataFrame,\n",
        "    valid_df: pl.DataFrame,\n",
        "    device: str,\n",
        "    hour: float,\n",
        "    metrics: list[str]\n",
        ") -> list[dict[str, Any]]:\n",
        "    num_ranked_items = max([int(x.split(\"@\")[1]) for x in metrics])\n",
        "\n",
        "    max_timestamp = train_df.select(pl.col(\"timestamp\").max()).item()\n",
        "    user_ids = train_df.select(\"uid\").unique()[\"uid\"].to_torch().to(device)\n",
        "\n",
        "    targets = Targets.from_sequential(\n",
        "        valid_df.group_by('uid', maintain_order=True).agg(\"item_id\"),\n",
        "        device,\n",
        "    )\n",
        "\n",
        "    item_embeddings = training_popular(\n",
        "        train_df=train_df,\n",
        "        hour=hour,\n",
        "        max_timestamp=max_timestamp,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    ranked = Ranked(\n",
        "        user_ids=user_ids,\n",
        "        item_ids=item_embeddings.ids[torch.topk(item_embeddings.embeddings, num_ranked_items, dim=0).indices]\n",
        "        .ravel()\n",
        "        .expand((user_ids.shape[0], num_ranked_items)),\n",
        "        num_item_ids=item_embeddings.ids.shape[0],\n",
        "    )\n",
        "\n",
        "    return calc_metrics(ranked, targets, metrics)\n",
        "\n",
        "\n",
        "def popularity(\n",
        "    train_df: pl.DataFrame,\n",
        "    valid_df: pl.DataFrame,\n",
        "    device: str,\n",
        "    hour: float,\n",
        "    report_metrics: list[str],\n",
        ") -> dict[str, Any]:\n",
        "    return evaluation_popular(train_df, valid_df, device, hour, report_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba069cdd",
      "metadata": {
        "id": "ba069cdd"
      },
      "outputs": [],
      "source": [
        "most_pop_metrics = popularity(\n",
        "    train_df=train_likes_df,\n",
        "    valid_df=test_likes_df,\n",
        "    device='cuda',\n",
        "    hour=0,\n",
        "    report_metrics=Constants.METRICS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82056159",
      "metadata": {
        "id": "82056159"
      },
      "outputs": [],
      "source": [
        "decay_pop_metrics = popularity(\n",
        "    train_df=train_likes_df,\n",
        "    valid_df=test_likes_df,\n",
        "    device='cuda',\n",
        "    hour=3,\n",
        "    report_metrics=Constants.METRICS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for metric_name, values in most_pop_metrics.items():\n",
        "  for k, value in values.items():\n",
        "    print(\n",
        "        f'{metric_name}@{k}',\n",
        "        round(value, 4),\n",
        "        Constants.IDEAL_METRICS['most_pop'][f'{metric_name}@{k}']\n",
        "    )"
      ],
      "metadata": {
        "id": "nxjlr17VX4xV"
      },
      "id": "nxjlr17VX4xV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for metric_name, values in decay_pop_metrics.items():\n",
        "  for k, value in values.items():\n",
        "    print(\n",
        "        f'{metric_name}@{k}',\n",
        "        round(value, 4),\n",
        "        Constants.IDEAL_METRICS['decay_pop'][f'{metric_name}@{k}']\n",
        "    )"
      ],
      "metadata": {
        "id": "I3qo4fuCYvgj"
      },
      "id": "I3qo4fuCYvgj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7f4c212a",
      "metadata": {
        "id": "7f4c212a"
      },
      "source": [
        "### ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4646d311",
      "metadata": {
        "id": "4646d311"
      },
      "outputs": [],
      "source": [
        "def process_train_data(train_df: pl.DataFrame) -> tuple[pl.DataFrame, list[int], list[int]]:\n",
        "    unique_pairs = train_df.select(\"uid\", \"item_id\").unique()\n",
        "\n",
        "    unique_uids = train_df.select(\"uid\").unique().sort(\"uid\")[\"uid\"].to_list()\n",
        "    unique_item_ids = train_df.select(\"item_id\").unique().sort(\"item_id\")[\"item_id\"].to_list()\n",
        "\n",
        "    return unique_pairs, unique_uids, unique_item_ids\n",
        "\n",
        "\n",
        "def build_csr_matrix(pairs: pl.DataFrame, unique_uids: list[int], unique_item_ids: list[int]) -> sp.csr_matrix:\n",
        "    uid_to_idx = {uid: i for i, uid in enumerate(unique_uids)}\n",
        "    item_id_to_idx = {item_id: i for i, item_id in enumerate(unique_item_ids)}\n",
        "\n",
        "    pairs = pairs.select(\n",
        "        pl.col(\"uid\").replace_strict(uid_to_idx, return_dtype=pl.UInt32),\n",
        "        pl.col(\"item_id\").replace_strict(item_id_to_idx, return_dtype=pl.UInt32),\n",
        "    )\n",
        "\n",
        "    rows, cols = pairs[\"uid\"].to_numpy(), pairs[\"item_id\"].to_numpy()\n",
        "    values = np.ones_like(rows, dtype=np.int32)\n",
        "\n",
        "    return sp.coo_matrix(\n",
        "        (values, (rows, cols)), dtype=np.float32, shape=(len(unique_uids), len(unique_item_ids))\n",
        "    ).tocsr()\n",
        "\n",
        "\n",
        "def train_embbedings_with_als(\n",
        "    user_item_interactions: sp.csr_matrix,\n",
        "    regularization: float = 0.01,\n",
        "    iterations: int = 100,\n",
        "    random_state: int = 42,\n",
        "    factors: int = 64,\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    from implicit.gpu.als import AlternatingLeastSquares\n",
        "    als = AlternatingLeastSquares(\n",
        "        factors=factors,\n",
        "        regularization=regularization,\n",
        "        iterations=iterations,\n",
        "        random_state=random_state,\n",
        "        calculate_training_loss=False,\n",
        "    )\n",
        "    als.fit(user_item_interactions, show_progress=False)\n",
        "    return als.user_factors.to_numpy(), als.item_factors.to_numpy()\n",
        "\n",
        "\n",
        "def calc_embeddings_metrics(\n",
        "    user_emb: np.ndarray,\n",
        "    item_emb: np.ndarray,\n",
        "    uid_tensor: torch.Tensor,\n",
        "    item_id_tensor: torch.Tensor,\n",
        "    targets: Targets,\n",
        "    metrics: list[str],\n",
        "    device: str,\n",
        ") -> dict[str, dict[int, float]]:\n",
        "    num_ranked_items = max([int(x.split(\"@\")[1]) for x in metrics])\n",
        "    user_emb = Embeddings(uid_tensor, torch.from_numpy(user_emb).to(device))\n",
        "    item_emb = Embeddings(item_id_tensor, torch.from_numpy(item_emb).to(device))\n",
        "\n",
        "    ranked_items = rank_items(user_emb, item_emb, num_ranked_items)\n",
        "    return calc_metrics(ranked_items, targets, metrics)\n",
        "\n",
        "\n",
        "def als(\n",
        "    train_df: pl.DataFrame,\n",
        "    valid_df: pl.DataFrame,\n",
        "    random_seeds: list[int],\n",
        "    device: str,\n",
        "    report_metrics: list[str],\n",
        "    hp: dict[str, Any] = {}\n",
        ") -> dict[str, dict[int, float]]:\n",
        "    train_pairs, unique_uids, unique_item_ids = process_train_data(train_df)\n",
        "    user_item_interactions = build_csr_matrix(train_pairs, unique_uids, unique_item_ids)\n",
        "\n",
        "    targets = Targets.from_sequential(\n",
        "        valid_df.group_by(\"uid\").agg(\"item_id\"),\n",
        "        device,\n",
        "    )\n",
        "\n",
        "    metrics_list = []\n",
        "\n",
        "    for seed in random_seeds:\n",
        "        user_emb, item_emb = train_embbedings_with_als(\n",
        "            user_item_interactions=user_item_interactions,\n",
        "            regularization=hp.get(\"regularization\", 0.01),\n",
        "            iterations=hp.get(\"iterations\", 100),\n",
        "            random_state=seed\n",
        "        )\n",
        "\n",
        "        metrics = calc_embeddings_metrics(\n",
        "            user_emb,\n",
        "            item_emb,\n",
        "            torch.tensor(unique_uids, device=device),\n",
        "            torch.tensor(unique_item_ids, device=device),\n",
        "            targets,\n",
        "            report_metrics,\n",
        "            device,\n",
        "        )\n",
        "        metrics_list.append(metrics)\n",
        "\n",
        "    return mean_dicts(metrics_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db9a077",
      "metadata": {
        "id": "5db9a077"
      },
      "outputs": [],
      "source": [
        "als(\n",
        "    train_df=train_likes_df,\n",
        "    test_df=test_likes_df,\n",
        "    random_seeds=[42, 1337, 15978, 1234321],\n",
        "    report_metrics=Constants.METRICS,\n",
        "    device='cuda'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc49a41c",
      "metadata": {
        "id": "fc49a41c"
      },
      "source": [
        "### ItemKNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6c3460",
      "metadata": {
        "id": "5f6c3460"
      },
      "outputs": [],
      "source": [
        "def eliminate_zeros(x: torch.Tensor, threshold: float = 1e-9) -> torch.Tensor:\n",
        "    mask = (x._values() > threshold).nonzero()\n",
        "    nv = x._values().index_select(0, mask.view(-1))\n",
        "    ni = x._indices().index_select(1, mask.view(-1))\n",
        "    return torch.sparse_coo_tensor(ni, nv, x.shape)\n",
        "\n",
        "\n",
        "def create_weighted_sparse_tensor(train: pl.DataFrame, tau: float) -> torch.Tensor:\n",
        "    uid_mapping = (\n",
        "        train.select(\"uid\").unique().with_columns(pl.col(\"uid\").rank(method=\"dense\").alias(\"uid_idx\") - 1)\n",
        "    )\n",
        "\n",
        "    item_mapping = (\n",
        "        train.select(\"item_id\")\n",
        "        .unique()\n",
        "        .with_columns(pl.col(\"item_id\").rank(method=\"dense\").alias(\"item_idx\") - 1)\n",
        "    )\n",
        "\n",
        "    processed = (\n",
        "        train.with_columns(pl.max(\"timestamp\").over(\"uid\").alias(\"max_timestamp\"))\n",
        "        .with_columns((pl.col(\"max_timestamp\") - pl.col(\"timestamp\")).alias(\"delta\"))\n",
        "        .with_columns((tau ** pl.col(\"delta\")).alias(\"weight\"))\n",
        "        .join(uid_mapping, on=\"uid\", how=\"inner\")\n",
        "        .join(item_mapping, on=\"item_id\", how=\"inner\")\n",
        "    )\n",
        "\n",
        "    coo_data = processed.group_by([\"uid_idx\", \"item_idx\"]).agg(pl.sum(\"weight\").alias(\"total_weight\"))\n",
        "\n",
        "    indices = torch.concat([coo_data[\"uid_idx\"].to_torch()[None, :], coo_data[\"item_idx\"].to_torch()[None, :]], dim=0)\n",
        "    values = torch.tensor(coo_data[\"total_weight\"].to_numpy(), dtype=torch.float)\n",
        "\n",
        "    return eliminate_zeros(\n",
        "        torch.sparse_coo_tensor(\n",
        "            indices=indices, values=values, size=(uid_mapping[\"uid_idx\"].max() + 1, item_mapping[\"item_idx\"].max() + 1)\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def sparse_normalize(sparse_tensor: torch.Tensor, dim=0, eps=1e-12):\n",
        "    indices = sparse_tensor.coalesce().indices()\n",
        "    values = sparse_tensor.coalesce().values()\n",
        "\n",
        "    unique_dim_indices, inverse = torch.unique(indices[dim], return_inverse=True)\n",
        "    squared_values = values**2\n",
        "    sum_squared = torch.zeros_like(unique_dim_indices, dtype=torch.float32)\n",
        "    sum_squared.scatter_add_(0, inverse, squared_values)\n",
        "\n",
        "    norms = torch.sqrt(sum_squared + eps)\n",
        "    normalized_values = values / norms[inverse]\n",
        "\n",
        "    return torch.sparse_coo_tensor(indices, normalized_values, sparse_tensor.size())\n",
        "\n",
        "\n",
        "def training(\n",
        "    train: pl.DataFrame,\n",
        "    hour: float,\n",
        "    user_item: torch.Tensor,\n",
        "    user_ids: torch.Tensor,\n",
        "    device: str,\n",
        "    decay: float = 0.9\n",
        ") -> Embeddings:\n",
        "    tau = 0.0 if hour == 0 else decay ** (1 / 24 / 60 / 60 / (hour / 24))\n",
        "\n",
        "    user_item_with_tau = create_weighted_sparse_tensor(train, tau)\n",
        "    user_embeddings = (user_item_with_tau @ user_item.T).to_dense()\n",
        "    user_embeddings = torch.nn.functional.normalize(user_embeddings, dim=-1)\n",
        "\n",
        "    return Embeddings(user_ids, user_embeddings.to(device))\n",
        "\n",
        "\n",
        "def item_knn(\n",
        "    train_df: pl.DataFrame,\n",
        "    valid_df: pl.DataFrame,\n",
        "    device: str,\n",
        "    hour: float,\n",
        "    report_metrics: list[str]\n",
        ") -> list[dict[str, Any]]:\n",
        "    num_ranked_items = max([int(x.split(\"@\")[1]) for x in report_metrics])\n",
        "\n",
        "    unique_user_ids = train_df.select(\"uid\").unique().sort(\"uid\")[\"uid\"].to_torch().to(device)\n",
        "    unique_item_ids = (\n",
        "        train_df.select(\"item_id\").unique().sort(\"item_id\")[\"item_id\"].to_torch().to(device)\n",
        "    )\n",
        "\n",
        "    user_item = create_weighted_sparse_tensor(train_df, 1.0)\n",
        "    item_embeddings = sparse_normalize(user_item.T.to(device), dim=-1)\n",
        "    item_embeddings = Embeddings(unique_item_ids, item_embeddings)\n",
        "\n",
        "    targets = Targets.from_sequential(\n",
        "        valid_df.group_by(\"uid\").agg(\"item_id\"),\n",
        "        device,\n",
        "    )\n",
        "\n",
        "    user_embeddings = training(\n",
        "        train=train_df,\n",
        "        hour=hour,\n",
        "        user_item=user_item,\n",
        "        user_ids=unique_user_ids,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    ranked = rank_items(\n",
        "        users=user_embeddings,\n",
        "        items=item_embeddings,\n",
        "        num_items=num_ranked_items,\n",
        "        batch_size=128,\n",
        "    )\n",
        "\n",
        "    return calc_metrics(ranked, targets, report_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e0d76b",
      "metadata": {
        "id": "e9e0d76b"
      },
      "outputs": [],
      "source": [
        "item_knn_metrics = item_knn(\n",
        "    train_df=train_likes_df,\n",
        "    valid_df=test_likes_df,\n",
        "    device='cuda',\n",
        "    hour=1.5,\n",
        "    report_metrics=Constants.METRICS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for metric_name, values in item_knn_metrics.items():\n",
        "  for k, value in values.items():\n",
        "    print(\n",
        "        f'{metric_name}@{k}',\n",
        "        round(value, 4),\n",
        "        Constants.IDEAL_METRICS['item_knn'][f'{metric_name}@{k}']\n",
        "    )"
      ],
      "metadata": {
        "id": "yL-BkaANZDWZ"
      },
      "id": "yL-BkaANZDWZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c796cc19",
      "metadata": {
        "id": "c796cc19"
      },
      "source": [
        "### EASE (SANSA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e04e5c71",
      "metadata": {
        "id": "e04e5c71"
      },
      "outputs": [],
      "source": [
        "def sansa(\n",
        "    train_df: pl.DataFrame,\n",
        "    valid_df: pl.DataFrame,\n",
        "    device: str,\n",
        "    report_metrics: list[str],\n",
        ") -> dict[str, Any]:\n",
        "    grouped_test, train, test = get_train_val_test_matrices(\n",
        "        train_df, valid_df\n",
        "    )\n",
        "\n",
        "    model = get_sansa_model()\n",
        "    model.fit(train)\n",
        "\n",
        "    unique_item_ids = (\n",
        "        train_df.select(\"item_id\").unique().sort(\"item_id\")[\"item_id\"].to_torch().to(device)\n",
        "    )\n",
        "\n",
        "    calculated_metrics = evaluate_sansa(\n",
        "        num_items_for_metrics=unique_item_ids.shape[0],\n",
        "        model=model,\n",
        "        device=device,\n",
        "        report_metrics=report_metrics,\n",
        "        grouped_test=grouped_test,\n",
        "        sparse_train=train,\n",
        "        sparse_test=test,\n",
        "    )\n",
        "\n",
        "    return calculated_metrics\n",
        "\n",
        "\n",
        "def get_train_val_test_matrices(\n",
        "    flat_train: pl.DataFrame,\n",
        "    flat_test: pl.DataFrame\n",
        ") -> tuple[pl.LazyFrame, pl.LazyFrame, sp.csr_matrix, sp.csr_matrix]:\n",
        "    all_uids = set(flat_train.get_column(\"uid\").to_list())\n",
        "    all_items = set(flat_train.get_column(\"item_id\").to_list())\n",
        "\n",
        "    # Create mapping to create sparse matrix\n",
        "    uid_to_idx = {uid: i for i, uid in enumerate(all_uids)}\n",
        "    item_id_to_idx = {item_id: i for i, item_id in enumerate(all_items)}\n",
        "\n",
        "    sparse_train, _ = get_sparse_data(flat_train, uid_to_idx, item_id_to_idx)\n",
        "    sparse_test, grouped_test = get_sparse_data(flat_test, uid_to_idx, item_id_to_idx)\n",
        "\n",
        "    return grouped_test, sparse_train, sparse_test\n",
        "\n",
        "\n",
        "def get_sparse_data(\n",
        "    df: pl.LazyFrame, uid_to_idx: dict[int, int], item_id_to_idx: dict[int, int]\n",
        ") -> tuple[sp.csr_matrix, pl.LazyFrame]:\n",
        "    df = df.with_columns(\n",
        "        pl.col(\"uid\").replace_strict(uid_to_idx).alias(\"uid\"),\n",
        "        pl.col(\"item_id\").replace_strict(item_id_to_idx, default=len(item_id_to_idx)).alias(\"item_id\"),\n",
        "        pl.lit(1).alias(\"action\"),\n",
        "    )\n",
        "\n",
        "    grouped_df = df.group_by('uid', maintain_order=True).agg(\n",
        "        [pl.col('item_id').alias('item_id'), pl.col('action').alias('actions')]\n",
        "    )\n",
        "\n",
        "    rows = []\n",
        "    cols = []\n",
        "    values = []\n",
        "\n",
        "    for user_id, item_ids, actions in tqdm(grouped_df.select('uid', 'item_id', 'actions').rows()):\n",
        "        rows.extend([user_id] * len(item_ids))\n",
        "        cols.extend(item_ids)\n",
        "        values.extend(actions)\n",
        "\n",
        "    user_item_data = sp.csr_matrix(\n",
        "        (values, (rows, cols)),\n",
        "        dtype=np.float32,\n",
        "        shape=(len(uid_to_idx), len(item_id_to_idx) + 1),  # +1 for default unknown test items\n",
        "    )\n",
        "\n",
        "    return user_item_data, grouped_df\n",
        "\n",
        "\n",
        "def get_sansa_model():\n",
        "    factorizer_config = ICFGramianFactorizerConfig(\n",
        "        factorization_shift_step=1e-3,\n",
        "        factorization_shift_multiplier=2.0,\n",
        "    )\n",
        "\n",
        "    inverter_config = UMRUnitLowerTriangleInverterConfig(\n",
        "        scans=1,  # number of scans through all columns of the matrix\n",
        "        finetune_steps=5,  # number of finetuning steps, targeting worst columns\n",
        "    )\n",
        "\n",
        "    config = SANSAConfig(\n",
        "        l2=20.0,  # regularization strength\n",
        "        weight_matrix_density=5e-5,  # desired density of weights\n",
        "        gramian_factorizer_config=factorizer_config,  # factorizer configuration\n",
        "        lower_triangle_inverter_config=inverter_config,  # inverter configuration\n",
        "    )\n",
        "\n",
        "    model = SANSA(config)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_sansa(\n",
        "    num_items_for_metrics: int,\n",
        "    model: SANSA,\n",
        "    device: str,\n",
        "    report_metrics: list[str],\n",
        "    grouped_test: pl.LazyFrame,\n",
        "    sparse_train: sp.csr_matrix,\n",
        "    sparse_test: sp.csr_matrix,\n",
        ") -> dict[str, Any]:\n",
        "\n",
        "    test_targets = Targets.from_sequential(grouped_test, device=device)\n",
        "\n",
        "    train_pred_sparse = model.forward(sparse_train)\n",
        "\n",
        "    A = train_pred_sparse\n",
        "    num_users = A.shape[0]\n",
        "    num_items_k = 150\n",
        "\n",
        "    # 0 if there is no such item\n",
        "    top_items_idx = np.full((num_users, num_items_k), 0, dtype=int)\n",
        "\n",
        "    # -1 score if there is no such item\n",
        "    top_items_score = np.full((num_users, num_items_k), -1, dtype=A.data.dtype)\n",
        "\n",
        "    for row in tqdm(range(num_users)):\n",
        "        start, end = A.indptr[row], A.indptr[row + 1]\n",
        "        row_scores = A.data[start:end]\n",
        "        row_cols = A.indices[start:end]\n",
        "\n",
        "        if len(row_scores) == 0:\n",
        "            continue\n",
        "\n",
        "        k_here = min(num_items_k, len(row_scores))\n",
        "        top_k = heapq.nlargest(k_here, zip(row_scores, row_cols), key=lambda x: x[0])\n",
        "\n",
        "        # Fill in\n",
        "        for i, (score, idx) in enumerate(top_k):\n",
        "            top_items_idx[row, i] = idx\n",
        "            top_items_score[row, i] = score\n",
        "\n",
        "    user_ids = torch.arange(top_items_idx.shape[0], dtype=torch.int32, device=\"cpu\")\n",
        "    scores = torch.as_tensor(top_items_score, dtype=torch.float32, device=\"cpu\")\n",
        "    scores_indices = torch.as_tensor(top_items_idx, dtype=torch.long, device=\"cpu\")\n",
        "    targets = torch.as_tensor(sparse_test.toarray(), dtype=torch.bool, device=\"cpu\")\n",
        "\n",
        "    targets = targets.to(dtype=torch.bool, device=device)\n",
        "    not_zero_user_indices = targets.any(dim=1)\n",
        "\n",
        "    not_zero_user_indices = not_zero_user_indices.to(dtype=torch.bool, device=\"cpu\")\n",
        "\n",
        "    user_ids = user_ids[not_zero_user_indices]\n",
        "    scores = scores[not_zero_user_indices]\n",
        "\n",
        "    scores_indices = scores_indices[not_zero_user_indices]\n",
        "\n",
        "    test_ranked = Ranked(\n",
        "        user_ids=user_ids.to(device),\n",
        "        scores=scores.to(device),\n",
        "        item_ids=scores_indices.to(device),\n",
        "        num_item_ids=num_items_for_metrics,\n",
        "    )\n",
        "\n",
        "    calculated_metrics = calc_metrics(test_ranked, test_targets, report_metrics)\n",
        "\n",
        "    return calculated_metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sansa_metrics = sansa(\n",
        "    train_df=train_likes_df,\n",
        "    valid_df=test_likes_df,\n",
        "    device='cuda',\n",
        "    report_metrics=Constants.METRICS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URLOCRRfJIzh",
        "outputId": "32b408c8-d0e8-4cad-ad59-e0eca9c52fa0"
      },
      "id": "URLOCRRfJIzh",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8273/8273 [00:00<00:00, 139952.80it/s]\n",
            "100%|██████████| 1301/1301 [00:00<00:00, 915568.71it/s]\n",
            "100%|██████████| 8273/8273 [03:01<00:00, 45.57it/s]\n",
            "Making target mask: 100%|██████████| 1301/1301 [00:00<00:00, 13027.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for metric_name, values in sansa_metrics.items():\n",
        "  for k, value in values.items():\n",
        "    print(\n",
        "        f'{metric_name}@{k}',\n",
        "        round(value, 4),\n",
        "        Constants.IDEAL_METRICS['sansa'][f'{metric_name}@{k}']\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhm8ViCxakYm",
        "outputId": "2837343d-054f-422d-9af1-37208a38f8a4"
      },
      "id": "yhm8ViCxakYm",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ndcg@10 0.0071 0.0068\n",
            "ndcg@100 0.0236 0.0203\n",
            "recall@10 0.0117 0.0105\n",
            "recall@100 0.0648 0.0616\n",
            "coverage@10 0.0161 0.0194\n",
            "coverage@100 0.1165 0.1601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cVzGBuPY7ZdP"
      },
      "id": "cVzGBuPY7ZdP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7f4c212a"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}